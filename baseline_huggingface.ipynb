{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face NPZ Dataset with Train/Val/Test Split\n",
    "\n",
    "This notebook contains the entire pipeline for training a PyTorch model based on a `.npz` image patch file stored in Hugging Face and a local `metadata/label.csv`. It also splits it into train/validation/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset index creation function and split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31674ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_index(repo_id, label_csv_path):\n",
    "    df = pd.read_csv(label_csv_path)\n",
    "    filename_to_label = dict(zip(df['pub_subspec_id'], df['label']))\n",
    "    data_index = []\n",
    "\n",
    "    for fname, label in filename_to_label.items():\n",
    "        fname_with_ext = fname if fname.endswith(\".npz\") else f\"{fname}.npz\"\n",
    "        url = f\"https://huggingface.co/datasets/{repo_id}/resolve/main/{fname_with_ext}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            npz = np.load(BytesIO(response.content))\n",
    "            for key in npz.files:\n",
    "                data_index.append((url, key, label))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {fname_with_ext}: {e}\")\n",
    "    return data_index\n",
    "\n",
    "def stratified_split(data_index, train_ratio=0.7, val_ratio=0.15, seed=316):\n",
    "    label_to_items = defaultdict(list)\n",
    "    for item in data_index:\n",
    "        label = item[2]\n",
    "        label_to_items[label].append(item)\n",
    "\n",
    "    train, val, test = [], [], []\n",
    "    random.seed(seed)\n",
    "\n",
    "    for label, items in label_to_items.items():\n",
    "        random.shuffle(items)\n",
    "        n_total = len(items)\n",
    "        n_train = int(n_total * train_ratio)\n",
    "        n_val = int(n_total * val_ratio)\n",
    "        train.extend(items[:n_train])\n",
    "        val.extend(items[n_train:n_train + n_val])\n",
    "        test.extend(items[n_train + n_val:])\n",
    "\n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ffdd27",
   "metadata": {},
   "source": [
    "## Dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349c6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceNPZWithLabelDataset(Dataset):\n",
    "    def __init__(self, data_index, transform=None):\n",
    "        self.data_index = data_index\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        url, key, label = self.data_index[idx]\n",
    "        response = requests.get(url)\n",
    "        npz = np.load(BytesIO(response.content))\n",
    "        patch = npz[key]\n",
    "        if patch.ndim == 2:\n",
    "            patch = Image.fromarray(patch.astype(np.uint8), mode='L')\n",
    "        elif patch.shape[-1] == 3:\n",
    "            patch = Image.fromarray(patch.astype(np.uint8), mode='RGB')\n",
    "        else:\n",
    "            patch = Image.fromarray(patch.astype(np.uint8))\n",
    "        if self.transform:\n",
    "            patch = self.transform(patch)\n",
    "        return patch, int(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb3b82",
   "metadata": {},
   "source": [
    "## Load and split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed3ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"nayoungku1/npz-histopathology-dataset\"\n",
    "label_csv_path = \"./metadata/label.csv\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "all_index = make_data_index(repo_id, label_csv_path)\n",
    "train_index, val_index, test_index = stratified_split(all_index, train_ratio=0.7, val_ratio=0.15, seed=314)\n",
    "\n",
    "train_dataset = HuggingFaceNPZWithLabelDataset(train_index, transform=transform)\n",
    "val_dataset = HuggingFaceNPZWithLabelDataset(val_index, transform=transform)\n",
    "test_dataset = HuggingFaceNPZWithLabelDataset(test_index, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN model definition and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 56 * 56, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))\n",
    "\n",
    "num_classes = len(pd.read_csv(label_csv_path)['label'].unique())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run learning loop (train + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
