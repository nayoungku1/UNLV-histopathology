{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ad36f887",
      "metadata": {
        "id": "ad36f887"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "from io import BytesIO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cd62d291",
      "metadata": {
        "id": "cd62d291"
      },
      "outputs": [],
      "source": [
        "\n",
        "# CSV 파일 경로와 Huggingface repo_id\n",
        "label_csv_path = \"./label.csv\"  # CSV 파일 경로 수정 필요\n",
        "repo_id = \"nayoungku1/npz-histopathology-dataset\"     # Huggingface repo 경로 수정 필요\n",
        "\n",
        "df = pd.read_csv(label_csv_path)\n",
        "df['pub_subspec_id'] = df['pub_subspec_id'].apply(lambda x: x if x.endswith('.npz') else f\"{x}.npz\")\n",
        "\n",
        "# stratified 샘플링 (총 46개 샘플 선택)\n",
        "_, selected_df = train_test_split(\n",
        "    df,\n",
        "    test_size=31,\n",
        "    stratify=df['label'],\n",
        "    random_state=314\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b05c927e",
      "metadata": {
        "id": "b05c927e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_stratified_data_index(repo_id, filtered_df):\n",
        "    filename_to_label = dict(zip(filtered_df['pub_subspec_id'], filtered_df['label']))\n",
        "    data_index = []\n",
        "\n",
        "    for fname, label in filename_to_label.items():\n",
        "        url = f\"https://huggingface.co/datasets/{repo_id}/resolve/main/{fname}\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            npz = np.load(BytesIO(response.content))\n",
        "            for key in npz.files:\n",
        "                patch_array = npz[key]\n",
        "                if patch_array.ndim == 4:\n",
        "                    for i in range(patch_array.shape[0]):\n",
        "                        data_index.append((url, key, i, label))\n",
        "                else:\n",
        "                    data_index.append((url, key, None, label))\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load {fname}: {e}\")\n",
        "    return data_index\n",
        "\n",
        "data_index = make_stratified_data_index(repo_id, selected_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5552ed21",
      "metadata": {
        "id": "5552ed21"
      },
      "outputs": [],
      "source": [
        "\n",
        "def stratified_split(data_index, train_ratio=0.7, val_ratio=0.15, seed=314):\n",
        "    label_to_items = defaultdict(list)\n",
        "    for item in data_index:\n",
        "        label = item[3]\n",
        "        label_to_items[label].append(item)\n",
        "\n",
        "    train, val, test = [], [], []\n",
        "    random.seed(seed)\n",
        "\n",
        "    for label, items in label_to_items.items():\n",
        "        random.shuffle(items)\n",
        "        n_total = len(items)\n",
        "        n_train = int(n_total * train_ratio)\n",
        "        n_val = int(n_total * val_ratio)\n",
        "        train.extend(items[:n_train])\n",
        "        val.extend(items[n_train:n_train + n_val])\n",
        "        test.extend(items[n_train + n_val:])\n",
        "    return train, val, test\n",
        "\n",
        "train_index, val_index, test_index = stratified_split(data_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a225a195",
      "metadata": {
        "id": "a225a195"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PatchDataset(Dataset):\n",
        "    def __init__(self, data_index, transform=None):\n",
        "        self.data_index = data_index\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        url, key, patch_idx, label = self.data_index[idx]\n",
        "        response = requests.get(url)\n",
        "        npz = np.load(BytesIO(response.content))\n",
        "        patch_array = npz[key]\n",
        "\n",
        "        patch = patch_array[patch_idx] if patch_idx is not None else patch_array\n",
        "        patch = Image.fromarray(patch.astype(np.uint8))\n",
        "\n",
        "        if self.transform:\n",
        "            patch = self.transform(patch)\n",
        "\n",
        "        return patch, int(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d8474dcf",
      "metadata": {
        "id": "d8474dcf"
      },
      "outputs": [],
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = PatchDataset(train_index, transform)\n",
        "val_dataset = PatchDataset(val_index, transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "st9eIdaUF7mA"
      },
      "id": "st9eIdaUF7mA",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for images, labels in train_loader:\n",
        "    print(\"Images shape:\", images.size())  # 기대: [256, 3, H, W]\n",
        "    print(\"Labels shape:\", labels.size())  # 기대: [256, 1]\n",
        "    break\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "i5O4cMvk4FPs",
        "outputId": "e73ca2c9-41f4-4510-d5ef-28e130e01fb5"
      },
      "id": "i5O4cMvk4FPs",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor images, labels in train_loader:\\n    print(\"Images shape:\", images.size())  # 기대: [256, 3, H, W]\\n    print(\"Labels shape:\", labels.size())  # 기대: [256, 1]\\n    break\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))  # 학습용 patch 총 개수\n",
        "print(len(val_dataset))    # 검증용 patch 총 개수"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoK7aQ7B47RU",
        "outputId": "522c98f6-3910-4dac-87d0-256fb6d2eb54"
      },
      "id": "qoK7aQ7B47RU",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9289\n",
            "1990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "95bf4e5b",
      "metadata": {
        "id": "95bf4e5b"
      },
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 32 * 32, batch_size)  # 64*64*64 -> 64*32*32로 수정\n",
        "        self.fc2 = nn.Linear(batch_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))  # [B, 32, 128, 128]\n",
        "        x = self.pool(torch.relu(self.conv2(x)))  # [B, 64, 64, 64]\n",
        "        x = x.view(-1, 64 * 64 * 64)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTAOgz6-3V5o",
        "outputId": "0f289b7e-11db-465d-ab0f-9d52281c7f6b"
      },
      "id": "TTAOgz6-3V5o",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNModel().to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "25FYGhnD3Yxn"
      },
      "id": "25FYGhnD3Yxn",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e52ea62d",
      "metadata": {
        "id": "e52ea62d"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "n_epochs = 5\n",
        "\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "accuracy_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        labels = labels.float().to(device) # Let's go back to this for now and print shapes\n",
        "        images = images.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)\n",
        "        print(f\"Images shape: {images.shape}, Labels shape: {labels.shape}, Logits shape: {logits.shape}\") # Add this line\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_history.append(train_loss)\n",
        "\n",
        "    # ====================\n",
        "    # Validation\n",
        "    # ====================\n",
        "    val_loss = 0.0\n",
        "    accuracy = 0.0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.float().to(device)\n",
        "\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = (logits >= 0.5).float()\n",
        "            corrects = (preds == labels).float().mean()\n",
        "            accuracy += corrects.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_loss_history.append(val_loss)\n",
        "\n",
        "    accuracy /= len(val_loader)\n",
        "    accuracy_history.append(accuracy)\n",
        "\n",
        "    # ====================\n",
        "    # Print metrics\n",
        "    # ====================\n",
        "    print(\n",
        "        f\"Epoch: {epoch+1:02}/{n_epochs:02} | \"\n",
        "        f\"Train loss: {train_loss:.3f} | \"\n",
        "        f\"Validation loss: {val_loss:.3f} | \"\n",
        "        f\"Accuracy: {accuracy:.3f}\"\n",
        "    )\n",
        "\n",
        "print(\"Elapsed: {:.2f} seconds\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "31dyqzwsKj-E"
      },
      "id": "31dyqzwsKj-E",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}