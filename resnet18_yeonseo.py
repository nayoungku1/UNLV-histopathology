# -*- coding: utf-8 -*-
"""ResNet18_yeonseo

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wZdfzXoeqVVF0GcI5y_EKEuyMCva9C0c
"""

import os, random
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from PIL import Image
from collections import defaultdict
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix, classification_report,
    roc_auc_score, roc_curve, balanced_accuracy_score,
    precision_score, recall_score, f1_score
)
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from torchvision.models import ResNet18_Weights

# âœ… í™˜ê²½ ì„¤ì •
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "6"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
use_gpu = torch.cuda.is_available()
print(f"âœ… Using device: {device}")

# âœ… ê²½ë¡œ
label_csv_path = "/home/s25piteam/UNLV-histopathology/ResNet/testing/label.csv"
data_dir = "/home/s25piteam/UNLV-histopathology/ResNet/npz_cache"
model_path = "./resnet18_best.pt"

assert os.path.exists(label_csv_path), f"{label_csv_path} not found"
assert os.path.exists(data_dir), f"{data_dir} not found"

# âœ… ë¼ë²¨ ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv(label_csv_path)
df['pub_subspec_id'] = df['pub_subspec_id'].apply(lambda x: x if x.endswith('.npz') else f"{x}.npz")
_, selected_df = train_test_split(df, test_size=31, stratify=df['label'], random_state=314)

# âœ… ì¸ë±ì‹±
def make_data_index(df):
    index = []
    for fname, label in zip(df['pub_subspec_id'], df['label']):
        path = os.path.join(data_dir, fname)
        if not os.path.exists(path): continue
        try:
            npz = np.load(path)
            for key in npz.files:
                arr = npz[key]
                if arr.ndim == 4:
                    for i in range(arr.shape[0]):
                        index.append((path, key, i, label))
                else:
                    index.append((path, key, None, label))
        except Exception as e:
            print(f"âš ï¸ Skip {path}: {e}")
    return index

data_index = make_data_index(selected_df)

# âœ… Stratified split
def stratified_split(index, train_ratio=0.7, val_ratio=0.15, seed=314):
    label_map = defaultdict(list)
    for item in index:
        label_map[item[3]].append(item)

    train, val, test = [], [], []
    random.seed(seed)
    for items in label_map.values():
        random.shuffle(items)
        n = len(items)
        train += items[:int(n*train_ratio)]
        val += items[int(n*train_ratio):int(n*(train_ratio+val_ratio))]
        test += items[int(n*(train_ratio+val_ratio)):]
    return train, val, test

train_idx, val_idx, test_idx = stratified_split(data_index)

# âœ… Dataset
class PatchDataset(Dataset):
    def __init__(self, index, transform=None):
        self.index = index
        self.transform = transform

    def __len__(self): return len(self.index)

    def __getitem__(self, i):
        path, key, i_patch, label = self.index[i]
        patch = np.load(path, allow_pickle=False)[key]
        patch = patch[i_patch] if i_patch is not None else patch
        patch = Image.fromarray(patch.astype(np.uint8))
        if self.transform: patch = self.transform(patch)
        return patch, int(label)

# âœ… Transform & DataLoader
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

num_workers = 1 if not use_gpu else 4
pin_memory = use_gpu

train_loader = DataLoader(PatchDataset(train_idx, transform), batch_size=32, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, drop_last=True)
val_loader = DataLoader(PatchDataset(val_idx, transform), batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)
test_loader = DataLoader(PatchDataset(test_idx, transform), batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)

# âœ… ResNet18 ëª¨ë¸
model = models.resnet18(weights=ResNet18_Weights.DEFAULT)
model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 1), nn.Sigmoid())
model = model.to(device)

criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
best_val_acc = 0

# âœ… í•™ìŠµ ë£¨í”„
for epoch in range(5):
    model.train()
    total_loss, correct, total = 0, 0, 0
    for x, y in train_loader:
        x, y = x.to(device), y.float().unsqueeze(1).to(device)
        optimizer.zero_grad()
        pred = model(x)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        pred_label = (pred > 0.5).float()
        correct += (pred_label == y).sum().item()
        total += y.size(0)
    train_acc = correct / total

    # Validation
    model.eval()
    val_loss, val_correct, val_total = 0, 0, 0
    with torch.no_grad():
        for x, y in val_loader:
            x, y = x.to(device), y.float().unsqueeze(1).to(device)
            pred = model(x)
            val_loss += criterion(pred, y).item()
            pred_label = (pred > 0.5).float()
            val_correct += (pred_label == y).sum().item()
            val_total += y.size(0)
    val_acc = val_correct / val_total

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), model_path)  # ëª¨ë¸ ì €ì¥

    print(f"[Epoch {epoch+1}] Train Loss: {total_loss/len(train_loader):.4f}, Acc: {train_acc:.4f} | Val Loss: {val_loss/len(val_loader):.4f}, Acc: {val_acc:.4f}")

print("âœ… í•™ìŠµ ì™„ë£Œ!")

# âœ… Test í‰ê°€
model.load_state_dict(torch.load(model_path))
model.eval()
all_preds, all_labels, all_probs = [], [], []

with torch.no_grad():
    for x, y in test_loader:
        x = x.to(device)
        probs = model(x).cpu().numpy()
        preds = (probs > 0.5).astype(int)
        all_preds.extend(preds.flatten())
        all_probs.extend(probs.flatten())
        all_labels.extend(y.numpy())

# âœ… Metric ì¶œë ¥
print("\nâœ… Classification Report")
print(classification_report(all_labels, all_preds, digits=4))

print(f"Balanced Accuracy: {balanced_accuracy_score(all_labels, all_preds):.4f}")
print(f"ROC AUC Score: {roc_auc_score(all_labels, all_probs):.4f}")

# âœ… Confusion Matrix
cm = confusion_matrix(all_labels, all_preds)
print("\nConfusion Matrix:\n", cm)

# âœ… ROC Curve ì‹œê°í™”
fpr, tpr, _ = roc_curve(all_labels, all_probs)
plt.figure()
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc_score(all_labels, all_probs):.4f})')
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("roc_curve.png")
plt.close()

print("ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜:", model_path)
print("ğŸ“Š ROC Curve ì €ì¥: roc_curve.png")