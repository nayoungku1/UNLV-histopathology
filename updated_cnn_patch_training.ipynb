{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6273b236",
   "metadata": {
    "id": "6273b236"
   },
   "source": [
    "# CNN 모델: 패치 단위 학습용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee3514",
   "metadata": {
    "id": "fbee3514"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f796ac5",
   "metadata": {
    "id": "3f796ac5"
   },
   "outputs": [],
   "source": [
    "def make_data_index(repo_id, label_csv_path):\n",
    "    df = pd.read_csv(label_csv_path)\n",
    "    filename_to_label = dict(zip(df['pub_subspec_id'], df['label']))\n",
    "    data_index = []\n",
    "\n",
    "    for fname, label in filename_to_label.items():\n",
    "        fname_with_ext = fname if fname.endswith(\".npz\") else f\"{fname}.npz\"\n",
    "        url = f\"https://huggingface.co/datasets/{repo_id}/resolve/main/{fname_with_ext}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            npz = np.load(BytesIO(response.content))\n",
    "            for key in npz.files:\n",
    "                patch_array = npz[key]\n",
    "                if patch_array.ndim == 4:\n",
    "                    for i in range(patch_array.shape[0]):\n",
    "                        data_index.append((url, key, i, label))\n",
    "                else:\n",
    "                    data_index.append((url, key, None, label))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {fname_with_ext}: {e}\")\n",
    "    return data_index\n",
    "\n",
    "\n",
    "def stratified_split(data_index, train_ratio=0.7, val_ratio=0.15, seed=42):\n",
    "    label_to_items = defaultdict(list)\n",
    "    for item in data_index:\n",
    "        label = item[2]\n",
    "        label_to_items[label].append(item)\n",
    "\n",
    "    train, val, test = [], [], []\n",
    "    random.seed(seed)\n",
    "\n",
    "    for label, items in label_to_items.items():\n",
    "        random.shuffle(items)\n",
    "        n_total = len(items)\n",
    "        n_train = int(n_total * train_ratio)\n",
    "        n_val = int(n_total * 0.15)\n",
    "        train.extend(items[:n_train])\n",
    "        val.extend(items[n_train:n_train + n_val])\n",
    "        test.extend(items[n_train + n_val:])\n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee064641",
   "metadata": {
    "id": "ee064641"
   },
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, data_index, transform=None):\n",
    "        self.data_index = data_index\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        url, key, patch_idx, label = self.data_index[idx]\n",
    "        response = requests.get(url)\n",
    "        npz = np.load(BytesIO(response.content))\n",
    "        patch_array = npz[key]\n",
    "\n",
    "        if patch_idx is not None:\n",
    "            patch = patch_array[patch_idx]\n",
    "        else:\n",
    "            patch = patch_array  # 이미 3D이면 그대로\n",
    "\n",
    "        if patch.ndim == 2:\n",
    "            patch = Image.fromarray(patch.astype(np.uint8), mode='L')\n",
    "        elif patch.shape[-1] == 3:\n",
    "            patch = Image.fromarray(patch.astype(np.uint8), mode='RGB')\n",
    "        else:\n",
    "            patch = Image.fromarray(patch.astype(np.uint8))\n",
    "\n",
    "        if self.transform:\n",
    "            patch = self.transform(patch)\n",
    "\n",
    "        return patch, int(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480cc079",
   "metadata": {
    "id": "480cc079"
   },
   "outputs": [],
   "source": [
    "repo_id = \"nayoungku1/npz-histopathology-dataset\"\n",
    "label_csv_path = \"./metadata/label.csv\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "all_index = make_data_index(repo_id, label_csv_path)\n",
    "train_idx, val_idx, test_idx = stratified_split(all_index)\n",
    "\n",
    "train_dataset = PatchDataset(train_idx, transform=transform)\n",
    "val_dataset = PatchDataset(val_idx, transform=transform)\n",
    "test_dataset = PatchDataset(test_idx, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd68c3-5b25-4b95-9730-f16b0400dc32",
   "metadata": {
    "id": "3cdd68c3-5b25-4b95-9730-f16b0400dc32"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 32 * 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))\n",
    "\n",
    "num_classes = len(pd.read_csv(label_csv_path)['label'].unique())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f18087-2dc1-4363-a0f5-cf320035b0d0",
   "metadata": {
    "id": "f6f18087-2dc1-4363-a0f5-cf320035b0d0"
   },
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total, correct, running_loss = 0, 0, 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, preds = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    print(f\"Epoch {epoch+1} | Train Acc: {correct/total:.2%}, Loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b23da0-2329-400d-a030-a85bd441495f",
   "metadata": {
    "id": "e6b23da0-2329-400d-a030-a85bd441495f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d63c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NPZPatchDataset(Dataset):\n",
    "    def __init__(self, npz_file_paths, labels, transform=None):\n",
    "        self.npz_file_paths = npz_file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.patch_info = []\n",
    "\n",
    "        for i, path in enumerate(self.npz_file_paths):\n",
    "            try:\n",
    "                data = np.load(path)\n",
    "                patches = data['patches']\n",
    "                for j in range(min(2000, patches.shape[0])):  # Limit per npz file\n",
    "                    self.patch_info.append((path, j, self.labels[i]))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {path}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patch_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, patch_idx, label = self.patch_info[idx]\n",
    "        data = np.load(path)\n",
    "        patch = data['patches'][patch_idx]\n",
    "        patch = Image.fromarray(patch.astype(np.uint8))\n",
    "\n",
    "        if self.transform:\n",
    "            patch = self.transform(patch)\n",
    "\n",
    "        return patch, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd9e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory containing .npz files\n",
    "npz_dir = \"./npz_data\"  # Change to your directory path\n",
    "all_npz_files = [f for f in os.listdir(npz_dir) if f.endswith('.npz')]\n",
    "selected_files = random.sample(all_npz_files, 46)\n",
    "\n",
    "file_paths = [os.path.join(npz_dir, fname) for fname in selected_files]\n",
    "labels = [1 if 'tumor' in fname else 0 for fname in selected_files]  # Modify if different labeling\n",
    "\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(file_paths, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762abf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = NPZPatchDataset(train_paths, train_labels, transform)\n",
    "val_dataset = NPZPatchDataset(val_paths, val_labels, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f3986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 32 * 32)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNModel().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
